# Python TaskGroup AI Agent

## ä½•ã‚’ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ / What This Project Does

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Python 3.11ä»¥é™ã®asyncio TaskGroupã‚’æ´»ç”¨ã—ã¦ã€è¤‡æ•°ã®AIã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ç‰¹ã«LLM APIï¼ˆOpenAI GPTã€Claudeã€Geminiãªã©ï¼‰ã¸ã®è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åŒæ™‚ã«å‡¦ç†ã—ã€å‡¦ç†æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã§ãã¾ã™ã€‚

ä¾‹ãˆã°ï¼š
- 10å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’é †æ¬¡å®Ÿè¡Œã™ã‚‹ã¨30ç§’ã‹ã‹ã‚‹å‡¦ç†ã‚’ã€ä¸¦è¡Œå®Ÿè¡Œã«ã‚ˆã‚Š8ç§’ã«çŸ­ç¸®
- ç•°ãªã‚‹ç¨®é¡ã®AIå‡¦ç†ï¼ˆç¿»è¨³ã€è¦ç´„ã€åˆ†æï¼‰ã‚’è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§åŒæ™‚å®Ÿè¡Œ
- å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚„ãƒãƒƒãƒå‡¦ç†ã‚’åŠ¹ç‡åŒ–

This project is a framework for efficiently executing multiple AI tasks concurrently using Python 3.11+ asyncio TaskGroup. It's particularly designed for processing multiple LLM API requests (OpenAI GPT, Claude, Gemini, etc.) simultaneously, significantly reducing processing time.

For example:
- Reduce 30-second sequential execution of 10 text generation tasks to 8 seconds with concurrent execution
- Execute different types of AI processing (translation, summarization, analysis) simultaneously with multiple agents
- Optimize large-scale data processing and batch operations

## ç‰¹å¾´ / Features

- ğŸš€ **ä¸¦è¡Œã‚¿ã‚¹ã‚¯å®Ÿè¡Œ / Concurrent Task Execution**: Python 3.11+ TaskGroupã‚’åŸºç›¤ã¨ã—ãŸåŠ¹ç‡çš„ãªä¸¦åˆ—å‡¦ç† / Built on Python 3.11+ TaskGroup for efficient parallel processing
- ğŸ¤– **æŸ”è»Ÿãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­è¨ˆ / Flexible Agent Architecture**: ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ã®ãŸã‚ã®æ‹¡å¼µå¯èƒ½ãªåŸºåº•ã‚¯ãƒ©ã‚¹ / Extensible base classes for custom agent implementations
- ğŸ”„ **ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿ / Multi-Agent Orchestration**: è¤‡é›‘ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æº / Coordinate multiple agents in complex pipelines
- âš¡ **éåŒæœŸãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆè¨­è¨ˆ / Async-First Design**: æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã®å®Œå…¨async/awaitã‚µãƒãƒ¼ãƒˆ / Full async/await support for optimal performance
- ğŸ›¡ï¸ **çµ„ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° / Built-in Error Handling**: TaskGroupã®ä¾‹å¤–ç®¡ç†ã«ã‚ˆã‚‹å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç† / Robust error handling with TaskGroup's exception management
- ğŸ“Š **ã‚¿ã‚¹ã‚¯è¿½è·¡ / Task Tracking**: ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€çµæœã€å®Ÿè¡Œæ™‚é–“ã®ç›£è¦– / Monitor task status, results, and execution times

## å¿…è¦ç’°å¢ƒ / Requirements

- Python 3.12 or higher
- Node.js 18+ (for frontend dashboard)
- Docker and Docker Compose
- uv (for dependency management)

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Option 1: Full Stack with Docker (Recommended) / Dockerå…¨ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆæ¨å¥¨ï¼‰

```bash
# Clone repository / ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/Daku-on/python-taskgroup-ai-agent.git
cd python-taskgroup-ai-agent

# One-command startup / ä¸€ç™ºèµ·å‹•
make full
# or / ã¾ãŸã¯
./scripts/start-full-stack.sh
```

**ğŸŒ Access URLs / ã‚¢ã‚¯ã‚»ã‚¹URL:**
- ğŸ“Š **Frontend Dashboard**: http://localhost:3000
- ğŸ”§ **Backend API**: http://localhost:8000  
- ğŸ“– **API Documentation**: http://localhost:8000/docs

### Option 2: Development Environment / é–‹ç™ºç’°å¢ƒ

```bash
# Install dependencies / ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
make install

# Start database only / ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿èµ·å‹•
make dev

# In separate terminals / åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§:
make backend   # Backend API server
make frontend  # Frontend development server
```

**ğŸŒ Access URLs (Dev) / ã‚¢ã‚¯ã‚»ã‚¹URLï¼ˆé–‹ç™ºï¼‰:**
- ğŸ“Š **Frontend**: http://localhost:5173
- ğŸ”§ **Backend**: http://localhost:8000

### Configuration / è¨­å®š

```bash
# Edit .env file / .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
cp .env.example .env
# Add your API keys / APIã‚­ãƒ¼ã‚’è¿½åŠ :
# OPENAI_API_KEY=your-key-here
# OPENAI_MODEL=gpt-3.5-turbo
```

### Available Commands / åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰

```bash
make help     # Show all commands / å…¨ã‚³ãƒãƒ³ãƒ‰è¡¨ç¤º
make full     # Start full stack / å…¨ã‚¹ã‚¿ãƒƒã‚¯èµ·å‹•  
make dev      # Development mode / é–‹ç™ºãƒ¢ãƒ¼ãƒ‰
make stop     # Stop all services / å…¨ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
make test     # Run tests / ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
make lint     # Code quality / ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
```

## Dashboard Features / ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½

### ğŸ“Š Web Dashboard
- **Task Execution**: Execute AI tasks with different agent types (LLM, Database, RAG)
- **Real-time Monitoring**: Live service status and performance metrics
- **Service Management**: Monitor health, metrics, and resource usage
- **WebSocket Updates**: Real-time notifications and status updates

### ğŸ¤– Agent Types / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ—
- **LLM Agent**: Direct OpenAI/LLM API calls
- **Database Agent**: PostgreSQL knowledge base search
- **RAG Agent**: Retrieval-Augmented Generation with smart decision making

## Programming Examples / ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ä¾‹

### Basic Agent Example

```python
import asyncio
from src.agent.base import BaseAgent, Task

class MyAgent(BaseAgent):
    async def process_task(self, task: Task):
        # Your task processing logic here
        result = await some_async_operation(task.data)
        return result

# Use the agent
async def main():
    agent = MyAgent(name="MyAgent", max_concurrent_tasks=10)
    
    tasks = [
        Task(id="1", name="Task 1", data={"prompt": "Hello"}),
        Task(id="2", name="Task 2", data={"prompt": "World"}),
    ]
    
    results = await agent.run_tasks(tasks)
    print(results)

asyncio.run(main())
```

### LLM Agent Example

```python
from src.agent.llm_agent import LLMAgent, LLMConfig

# Configure your LLM
config = LLMConfig(
    api_url="https://api.openai.com/v1/chat/completions",
    api_key="your-api-key",
    model="gpt-3.5-turbo"
)

# Create agent
async with LLMAgent("GPT-Agent", config) as agent:
    task = Task(
        id="1",
        name="Generate Story",
        data={"prompt": "Write a short story about AI"}
    )
    
    result = await agent.run_single_task(task)
    print(result.result)
```

### Multi-Agent Pipeline

```python
from src.agent.llm_agent import MultiAgentOrchestrator

orchestrator = MultiAgentOrchestrator()
orchestrator.register_agent(agent1)
orchestrator.register_agent(agent2)

pipeline = [
    {
        "agent": "agent1",
        "tasks": [{"id": "1", "name": "Analyze", "data": {...}}]
    },
    {
        "agent": "agent2", 
        "tasks": [{"id": "2", "name": "Generate", "data": {...}}]
    }
]

results = await orchestrator.run_pipeline(pipeline)
```

## Architecture

The framework is built around three core concepts:

1. **Tasks**: Represent units of work with unique IDs, names, and data payloads
2. **Agents**: Process tasks asynchronously with configurable concurrency limits
3. **TaskGroups**: Python 3.11+ feature for structured concurrency and error handling

## Examples

Check the `examples/` directory for more detailed examples:
- `simple_agent.py`: Basic agent implementation with simulated tasks
- `openai_agent.py`: Real OpenAI API integration example
- `rag_agent.py`: **RAG (Retrieval-Augmented Generation) with PostgreSQL knowledge base**

### ğŸš€ Quick Start: RAG Agent Demo

**ä¸€ç™ºã§å…¨éƒ¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ‡ãƒ¢å®Ÿè¡Œï¼š**
```bash
# è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & RAGãƒ‡ãƒ¢å®Ÿè¡Œ
uv run python scripts/start_demo.py
```

**æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:**
```bash
# 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•
docker-compose up -d

# 2. ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ä½œæˆ
uv run python database/setup_knowledge.py

# 3. RAGãƒ‡ãƒ¢å®Ÿè¡Œ
uv run python examples/rag_agent.py
```

### ğŸ¤– RAG Agent Features / RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½

The RAG agent demonstrates intelligent information retrieval:

RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è³¢ã„æƒ…å ±æ¤œç´¢ã‚’å®Ÿæ¼”ã—ã¾ã™ï¼š

- **ğŸ§  Smart Decision Making / è³¢ã„åˆ¤æ–­**: Automatically decides whether to fetch data from knowledge base / ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹ã‚’è‡ªå‹•åˆ¤æ–­
- **ğŸ“š Knowledge Base Search / ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢**: PostgreSQL full-text search with Claude Code documentation / Claude Codeãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å«ã‚€PostgreSQLå…¨æ–‡æ¤œç´¢
- **âš¡ Concurrent Processing / ä¸¦è¡Œå‡¦ç†**: Multiple questions processed simultaneously using TaskGroup / TaskGroupã‚’ä½¿ç”¨ã—ãŸè¤‡æ•°è³ªå•ã®åŒæ™‚å‡¦ç†
- **ğŸ¯ Context-Aware Responses / æ–‡è„ˆã‚’ç†è§£ã—ãŸå›ç­”**: Combines retrieved knowledge with LLM capabilities / æ¤œç´¢ã•ã‚ŒãŸçŸ¥è­˜ã¨LLMæ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›

**Example Questions / è³ªå•ä¾‹:**
- "Claude Codeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ" â†’ Uses knowledge base / ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ä½¿ç”¨
- "ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ" â†’ Direct LLM response / LLMç›´æ¥å›ç­”  
- "TaskGroupã®æ©Ÿèƒ½ã«ã¤ã„ã¦æ•™ãˆã¦" â†’ Uses knowledge base / ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ä½¿ç”¨

### ğŸ­ Service Orchestration / ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

**Advanced service-based architecture with workflow orchestration:**

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŒã¤é«˜åº¦ãªã‚µãƒ¼ãƒ“ã‚¹ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:**

```bash
# ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œ / Run orchestration demo
uv run python examples/orchestrator_demo.py

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ / Interactive dashboard
uv run python examples/service_dashboard.py
```

**ğŸ—ï¸ Service Architecture Features / ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ©Ÿèƒ½:**

- **ğŸ”§ Service Registry / ã‚µãƒ¼ãƒ“ã‚¹ç™»éŒ²**: Automatic service discovery and health monitoring / è‡ªå‹•ã‚µãƒ¼ãƒ“ã‚¹ç™ºè¦‹ã¨ãƒ˜ãƒ«ã‚¹ç›£è¦–
- **ğŸ­ Orchestration Engine / ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³**: Workflow coordination with dependency resolution / ä¾å­˜é–¢ä¿‚è§£æ±ºã‚’æŒã¤ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼èª¿æ•´
- **âš¡ Parallel & Sequential Execution / ä¸¦åˆ—ãƒ»ç›´åˆ—å®Ÿè¡Œ**: Mixed workflow patterns with TaskGroup / TaskGroupã‚’ä½¿ã£ãŸæ··åœ¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
- **ğŸ”„ Auto Retry & Error Handling / è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: Robust failure recovery mechanisms / å …ç‰¢ãªéšœå®³å›å¾©ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
- **ğŸ“Š Real-time Monitoring / ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: Service metrics and workflow tracking / ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¿½è·¡
- **ğŸ›ï¸ Management Dashboard / ç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: Interactive service management interface / ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

**Workflow Example / ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹:**
```python
workflow_steps = [
    {
        "step_id": "data_fetch",
        "service_name": "database-service", 
        "operation": "search",
        "depends_on": [],
        "parallel": True
    },
    {
        "step_id": "ai_analysis",
        "service_name": "rag-service",
        "operation": "question", 
        "depends_on": ["data_fetch"],
        "parallel": False
    }
]
```

## é–‹ç™º / Development

```bash
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ / Run tests
uv run pytest tests/

# ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ / Code formatting
uv run ruff format .

# ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚° / Linting
uv run ruff check .

# å‹ãƒã‚§ãƒƒã‚¯ / Type checking
uv run mypy src/

# å…¨å“è³ªãƒã‚§ãƒƒã‚¯ä¸€æ‹¬å®Ÿè¡Œ / Run all quality checks
uv run pytest tests/ && uv run ruff check . && uv run ruff format --check . && uv run mypy src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## æ—¥æœ¬èªè¦ç´„

Python 3.11ä»¥é™ã®TaskGroupã‚’æ´»ç”¨ã—ãŸAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚è¤‡æ•°ã®AI/LLM APIã‚³ãƒ¼ãƒ«ã‚’åŠ¹ç‡çš„ã«ä¸¦è¡Œå‡¦ç†ã™ã‚‹ãŸã‚ã®è¨­è¨ˆã€‚

ä¸»ãªç‰¹å¾´ï¼š
- TaskGroupã«ã‚ˆã‚‹ä¸¦è¡Œã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
- æ‹¡å¼µå¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å®Œå…¨ãªéåŒæœŸã‚µãƒãƒ¼ãƒˆ
- å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

åŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯ã€BaseAgentã‚’ç¶™æ‰¿ã—ã¦process_taskãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã€TaskGroupã§è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹ä»•çµ„ã¿ã€‚